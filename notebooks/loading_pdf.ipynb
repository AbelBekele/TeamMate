{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1343, which is longer than the specified 1000\n"
     ]
    }
   ],
   "source": [
    "loader = TextLoader(\"../data/Tenacious.txt\", encoding=\"windows-1252\")\n",
    "documents = loader.load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# loader = TextLoader(\"../data/Robinson_Advisory.txt\", encoding=\"windows-1252\")\n",
    "# index = VectorstoreIndexCreator().from_loaders([loader])\n",
    "\n",
    "# text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "#     chunk_size=1024, chunk_overlap=0\n",
    "# )\n",
    "# documents = loader.load_and_split()\n",
    "# docs = text_splitter.split_documents(documents)\n",
    "# embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate\n",
    "from langchain_weaviate.vectorstores import WeaviateVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "weaviate_client = weaviate.connect_to_local()\n",
    "db = WeaviateVectorStore.from_documents(docs, embeddings, client=weaviate_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 1:\n",
      "One of the most dangerous thoughts for a technical person is to make an\n",
      "uninformed or emotion based ...\n",
      "\n",
      "Document 2:\n",
      "The solution should focus on specific students tackling specific challenges, such\n",
      "as taking universi...\n",
      "\n",
      "Document 3:\n",
      "gv Tenacious\n",
      "Intelligence\n",
      "\n",
      "Corporation\n",
      "\n",
      "Tenacious Talent - GenAl\n",
      "UpSkilling\n",
      "\n",
      "GEN-AI Challenge\n",
      "TEAM-M...\n",
      "\n",
      "Document 4:\n",
      "3. Architectures:\n",
      "e Explore different agent architectures, such as rule-based,\n",
      "learning-based, and h...\n"
     ]
    }
   ],
   "source": [
    "query = \"What is this week challenge?\"\n",
    "docs = db.similarity_search(query)\n",
    "\n",
    "# Print the first 100 characters of each result\n",
    "for i, doc in enumerate(docs):\n",
    "    print(f\"\\nDocument {i+1}:\")\n",
    "    print(doc.page_content[:100] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1343, which is longer than the specified 1000\n"
     ]
    }
   ],
   "source": [
    "with open(\"../data/Tenacious.txt\", encoding=\"windows-1252\") as f:\n",
    "    state_of_the_union = f.read()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_text(state_of_the_union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "docsearch = WeaviateVectorStore.from_texts(\n",
    "    texts,\n",
    "    embeddings,\n",
    "    client=weaviate_client,\n",
    "    metadatas=[{\"source\": f\"{i}-pl\"} for i in range(len(texts))],\n",
    ")\n",
    "\n",
    "retriever = docsearch.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['context', 'question'] messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question}\\nContext: {context}\\nAnswer:\\n\"))]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "Question: {question}\n",
    "Context: {context}\n",
    "Answer:\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The challenge for this week is to focus on creating a personalized agent for optimizing trainee time and focus, as part of the Team-Mate project. The main objective is to enhance the training and educational experience by introducing a supportive digital assistant to help students manage tasks effectively and access necessary resources effortlessly. The deliverables include basic chatbot functionality, adaptive learning support, building an understanding of the trainee, setting up a RAG system, and creating a simple frontend interface for interaction.'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain.invoke(\"What is this week's challenge?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: filler question \\nContext: filler context \\nAnswer:\")]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import hub\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "example_messages = prompt.invoke(\n",
    "    {\"context\": \"filler context\", \"question\": \"filler question\"}\n",
    ").to_messages()\n",
    "\n",
    "example_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
      "Question: filler question \n",
      "Context: filler context \n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "print(example_messages[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The company involved in this project is TeamMate, an independent company dedicated to helping students and trainees manage their time and focus more effectively. They aim to optimize schedules, facilitate collaboration, and provide tailored support to enhance learning experiences and job readiness. TeamMate introduces a personalized LLM-enhanced agent to serve as a supportive digital assistant for students."
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "for chunk in rag_chain.stream(\"what company is invoved in this project?\"):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"This week's challenge is to focus on developing prompt engineering proficiency, implementing RAG systems, designing intelligent agents, implementing vector stores, and analyzing data with LLMs. Thanks for asking!\""
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Use three sentences maximum and keep the answer as concise as possible.\n",
    "Always say \"thanks for asking!\" at the end of the answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "custom_rag_prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | custom_rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain.invoke(\"What is this weeks challenge?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import (\n",
    "    PromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    ChatPromptTemplate,\n",
    ")\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "review_template_str = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Use three sentences maximum and keep the answer as concise as possible.\n",
    "Always say \"thanks for asking!\" at the end of the answer.\n",
    "\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "review_system_prompt = SystemMessagePromptTemplate(\n",
    "    prompt=PromptTemplate(\n",
    "        input_variables=[\"context\"],\n",
    "        template=review_template_str,\n",
    "    )\n",
    ")\n",
    "\n",
    "review_human_prompt = HumanMessagePromptTemplate(\n",
    "    prompt=PromptTemplate(\n",
    "        input_variables=[\"question\"],\n",
    "        template=\"{question}\",\n",
    "    )\n",
    ")\n",
    "messages = [review_system_prompt, review_human_prompt]\n",
    "\n",
    "review_prompt_template = ChatPromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    messages=messages,\n",
    ")\n",
    "\n",
    "chat_model = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "\n",
    "review_chain = review_prompt_template | chat_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Yes, I had a great stay! Thanks for asking!', response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 90, 'total_tokens': 102}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-e5c796fe-379c-429d-8c52-dfe20dc61425-0', usage_metadata={'input_tokens': 90, 'output_tokens': 12, 'total_tokens': 102})"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = \"I had a great stay!\"\n",
    "question = \"Did anyone have a positive experience?\"\n",
    "\n",
    "review_chain.invoke({\"context\": context, \"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser = StrOutputParser()\n",
    "\n",
    "review_chain = review_prompt_template | chat_model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yes, I had a great stay! Thanks for asking!'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = \"I had a great stay!\"\n",
    "question = \"Did anyone have a positive experience?\"\n",
    "\n",
    "review_chain.invoke({\"context\": context, \"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | review_prompt_template\n",
    "    | chat_model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The challenge this week is about creating a personalized agent called Team-Mate to help students and trainees manage their time and focus effectively in educational settings like university courses, training programs, or boot camps. The project aims to optimize schedules, facilitate collaboration, and provide tailored support to enhance learning experiences and job readiness. Thanks for asking!'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "question = \"\"\"What is this week challenge about?\"\"\"\n",
    "review_chain.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "def get_current_wait_time(job: str) -> int | str:\n",
    "    \"\"\"Dummy function to generate fake wait times\"\"\"\n",
    "\n",
    "    if job not in [\"Software Engineer\", \"B\", \"C\", \"D\"]:\n",
    "        return f\"job wait {job} does not exist\"\n",
    "\n",
    "    # Simulate API call delay\n",
    "    time.sleep(1)\n",
    "\n",
    "    return random.randint(0, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import (\n",
    "    create_openai_functions_agent,\n",
    "    Tool,\n",
    "    AgentExecutor,\n",
    ")\n",
    "from langchain import hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "Tool(\n",
    "    name=\"Documents\",\n",
    "    func=review_chain.invoke,\n",
    "    description=\"\"\"Useful when you need to answer questions\n",
    "    about uploaded documents. Not useful for answering questions\n",
    "    about specific For instance,\n",
    "    if the question is \"What does the document say about project deadlines?\",\n",
    "    the input should be \"What does the document say about project deadlines?\"\n",
    "    \"\"\",\n",
    "),\n",
    "\n",
    "Tool(\n",
    "    name=\"JobWait\",\n",
    "    func=get_current_wait_time,\n",
    "    description=\"\"\"Use when asked about current wait times to get a job.\n",
    "    This tool can only get the current wait time for a job application and does\n",
    "    not have any information about aggregate or historical wait times. This tool returns wait times in\n",
    "    days. Do not pass the word \"job\" as input, only the job title itself. For instance, if the question is\n",
    "    \"How long will I wait for a Software Engineer position?\", the input should be \n",
    "    \"What is the wait time for a Software Engineer position\".\n",
    "    \"\"\",\n",
    ")\n",
    "]\n",
    "\n",
    "agent_prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "\n",
    "agent_chat_model = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo-1106\",\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "agent = create_openai_functions_agent(\n",
    "    llm=agent_chat_model,\n",
    "    prompt=agent_prompt,\n",
    "    tools=tools,\n",
    ")\n",
    "\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    return_intermediate_steps=True,\n",
    "    verbose=True,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `JobWait` with `Software Engineer`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3m3037\u001b[0m\u001b[32;1m\u001b[1;3mThe current wait time for a Software Engineer position is approximately 3037 days.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'How long will I wait for a Software Engineer position?',\n",
       " 'output': 'The current wait time for a Software Engineer position is approximately 3037 days.',\n",
       " 'intermediate_steps': [(AgentActionMessageLog(tool='JobWait', tool_input='Software Engineer', log='\\nInvoking: `JobWait` with `Software Engineer`\\n\\n\\n', message_log=[AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"__arg1\":\"Software Engineer\"}', 'name': 'JobWait'}}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 247, 'total_tokens': 264}, 'model_name': 'gpt-3.5-turbo-1106', 'system_fingerprint': 'fp_b953e4de39', 'finish_reason': 'function_call', 'logprobs': None}, id='run-530c72e7-7ed2-4817-9a7f-6e1ec51d66d5-0', usage_metadata={'input_tokens': 247, 'output_tokens': 17, 'total_tokens': 264})]),\n",
       "   3037)]}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke( {\"input\": \"How long will I wait for a Software Engineer position?\"} )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
